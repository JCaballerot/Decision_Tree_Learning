{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvt697PAcpCnhlvvaQGqht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Decision_Tree_Learning/blob/main/0.%20Regression_trees/%C3%81rboles_de_Decisi%C3%B3n_en_Python_con_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.ctic.uni.edu.pe/wp-content/uploads/2022/04/588px-x-348px-web-1.png\" alt=\"HTML5 Icon\" width=\"900\" height=\"350\" >\n"
      ],
      "metadata": {
        "id": "WTeuiYwz_P2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1 align=center><font size = 5>Laboratorio: Árboles de Decisión en Python con Databricks\n",
        "</font></h1>\n"
      ],
      "metadata": {
        "id": "UwTLZ_gz9dIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "WbiHNens_XO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos\n",
        "- Entender el proceso de creación y evaluación de un modelo de Árbol de Decisión.\n",
        "- Practicar el preprocesamiento de datos incluyendo manejo de valores atípicos y muestreo.\n",
        "- Visualizar la importancia de las características y la estabilidad de los nodos en un árbol de decisión.\n",
        "- Aplicar y evaluar un modelo de Árbol de Decisión."
      ],
      "metadata": {
        "id": "SXbm16rL9exp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ct10fzef_YK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabla de Contenidos\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Introducción</a>  \n",
        "\n",
        "2. <a href=\"#item32\">Preprocesamiento de Datos</a>  \n",
        "    - Muestreo de la data\n",
        "    - Tratamiento de valores atípicos\n",
        "3. <a href=\"#item33\">Entrenamiento y visualización</a>  \n",
        "    - Entrenamiento del Árbol de Decisión\n",
        "    - Visualización del Árbol de Decisión\n",
        "4. <a href=\"#item34\">Evaluación del modelo</a>  \n",
        "    - Predicciones y evaluación del modelo\n",
        "    - Análisis de desviación porcentual\n",
        "5. <a href=\"#item34\">Importancia de Características y Estabilidad de Nodo</a>  \n",
        "    - Importancia de características\n",
        "    - Estabilidad de nodos\n",
        "6. <a href=\"#item34\">Árboles Interactivos</a>  \n",
        "    - Funciones para evaluar y visualizar cortes óptimos\n",
        "    - Evaluación y visualización del mejor corte\n",
        "\n",
        "</font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "cMM90_TY-Of5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "n1oFCXeO_WPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos un script externo ubicado en la ruta especificada para cargar funciones auxiliares.\n"
      ],
      "metadata": {
        "id": "j-KVbdrLChQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"/Workspace/Users/jcaballerot@bcp.com.pe/2024Q3 - Decision Trees/libs/aux_funs\""
      ],
      "metadata": {
        "id": "7-3Jc3DB_hAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las bibliotecas necesarias para crear gráficos y visualizaciones.\n"
      ],
      "metadata": {
        "id": "fo_Ka_-uCn9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "_dSOUV6-_uu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "--J0tqdVAD2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el archivo CSV en un DataFrame de pandas y mostramos las primeras filas para verificar los datos.\n"
      ],
      "metadata": {
        "id": "hMt-rqoZCuMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pddf = pd.read_csv('/Workspace/Users/jcaballerot@bcp.com.pe/2024Q3 - Decision Trees/data/HM_FLUJOS_fad_mvp2.csv', index_col = 0)\n",
        "pddf.head()\n"
      ],
      "metadata": {
        "id": "4VlUnTS8_wUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agrupamos los datos por la columna 'codmes_flujo' y contamos el número de ocurrencias en cada grupo.\n"
      ],
      "metadata": {
        "id": "YxGfcbGkDDII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pddf.groupby('codmes_flujo').size()"
      ],
      "metadata": {
        "id": "Vl0foSSR_0YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos el promedio y la cantidad de valores de 'fad' agrupados por 'codmes_flujo'."
      ],
      "metadata": {
        "id": "eKTI-V9XDIya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values  = pddf.groupby('codmes_flujo').fad.mean()\n",
        "count_values = pddf.groupby('codmes_flujo').fad.count()"
      ],
      "metadata": {
        "id": "QDlSZvjmDMBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crear una figura y un eje\n",
        "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "ax1.bar(count_values.index, count_values, color='lightgrey', alpha=0.5, label='Materialidad')\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Graficar la línea del promedio\n",
        "sns.lineplot(x=mean_values.index, y=mean_values, ax=ax2, color='blue', label='Average Value', marker='o')\n",
        "\n",
        "# Etiquetas y título\n",
        "ax1.set_xlabel('Codmes_flujo', fontsize = 10)\n",
        "ax1.set_ylabel('N', color='grey')\n",
        "ax2.set_ylabel('Average Value', color='blue')\n",
        "plt.title('FAD por Codmes flujo')\n",
        "\n",
        "# Ajustar la escala y los límites si es necesario\n",
        "ax1.set_ylim(0, 1800)\n",
        "ax2.set_ylim(0, 18000)\n",
        "\n",
        "ax1.tick_params(axis='y', labelsize=8)\n",
        "ax2.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "ax1.set_xticks(mean_values.index)\n",
        "ax1.set_xticklabels(mean_values.index, fontsize=8)\n",
        "\n",
        "# Mostrar leyendas\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ELTiI8l_5K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una lista de características que vamos a utilizar en el modelo.\n"
      ],
      "metadata": {
        "id": "KajM67uzDW-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "features = ['rat_pas_act_meanprev6_ant_per_med',\n",
        " 'mtoprincipalsol_rat_avg6_avg12_zn_pastrx_med',\n",
        " 'mtosaldopromediovigsol_1000_countprev12_ant_per_deu90k_med',\n",
        " 'mtocuotapresunta_meanprev12_seg_ratio',\n",
        " 'mtocuotapresunta_ant_ren_ratio',\n",
        " 'geo_ncc_cuota_tck_ae',\n",
        " 'geo_c3k_cuota_sum',\n",
        " 'geo_nt_fpas6_tck',\n",
        " 'isav_tkt_opec_pago_srv_prm_p6m',\n",
        " 'isav_tkt_opec_trnf_prm_p6m',\n",
        " 'isav_tkt_opec_pago_srv_sol_prm_u6m',\n",
        " 'isav_mto_opea_sav_ahs_prm_u12',\n",
        " 'can_mto_tmo_tot_sol_prm_u9m',\n",
        " 'can_tkt_tmo_tot_prm_u9m',\n",
        " 'can_tkt_tmo_tot_dig_prm_u12',\n",
        " 'can_mto_tmo_ven_sol_prm_u12',\n",
        " 'can_mto_tmo_bmo_prm_u3m',\n",
        " 'can_tkt_tmo_atm_ret_prm_u9m',\n",
        " 'pos_tkt_trx_td_prm_u9m',\n",
        " 'pos_mto_trx_g25_prm_u9m',\n",
        " 'can_ctd_tmo_tot_trf_dig_prm_u3m',\n",
        " 'rcc_mto_sld_py2_med_u12',\n",
        " 'monto_cosecha_pyme_meanprev12',\n",
        " 'dec_var_deuda_pyme_meanprev12',\n",
        " 'mtototalcargos_numcargos_rat_avg12_avg12_aggsum',\n",
        " 'mtototalabonos_numabonos_rat_avg12_avg12',\n",
        " 'numempleados',\n",
        " 'DEUDA_VIGENTE_S_HIP_meanPrev12',\n",
        " 'MTOSALDOPROMEDIOVIGSOL_meanPrev12']\n",
        ""
      ],
      "metadata": {
        "id": "1ZsTTdC7_sCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Sampling\n"
      ],
      "metadata": {
        "id": "tFjYWQYN_9ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determinamos las matrices de datos independientes (X) y dependiente (y)"
      ],
      "metadata": {
        "id": "C8ic7WVTDdck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pddf\n",
        "y = pddf.fad\n",
        ""
      ],
      "metadata": {
        "id": "IR-nxizNANMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos el muestreo de los datos, dividiéndolos en conjuntos de entrenamiento y prueba.\n"
      ],
      "metadata": {
        "id": "d8RQzlKGDkOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestreo de data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    stratify = X.codmes_flujo,\n",
        "                                                    test_size = 0.4,\n",
        "                                                    random_state = 123)\n",
        ""
      ],
      "metadata": {
        "id": "yJIqhk53DhHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear figura y subplot\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "sns.boxplot(data = y_train.values, orient=\"h\", ax=ax)\n",
        "ax.set_title('FAD')"
      ],
      "metadata": {
        "id": "-zywqIvqAOw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función para tratar valores atípicos utilizando el rango intercuartílico (IQR).\n"
      ],
      "metadata": {
        "id": "BLbsL32VDrHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def treat_outlier_IQR(series):\n",
        "    # Calculate the Interquartile Range (IQR)\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the limits for outliers\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Cap the extreme values to the defined limits\n",
        "    treated_series = series.clip(lower_limit, upper_limit)\n",
        "    print(f'Valid range: de {lower_limit} a {upper_limit}')\n",
        "    return treated_series"
      ],
      "metadata": {
        "id": "RAmupVEBARo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t = treat_outlier_IQR(y_train)\n"
      ],
      "metadata": {
        "id": "S5GliSHdAdaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t = y_train.clip(100, 30736.5)\n",
        "y_test_t  = y_test.clip(100, 30736.5)"
      ],
      "metadata": {
        "id": "dZU3JrB9Aeif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear figura y subplots\n",
        "fig, axes = plt.subplots(2, 1, figsize=(6, 4))\n",
        "\n",
        "ax1 = axes[0]\n",
        "sns.boxplot(data=y_test.values, orient=\"h\", ax=ax1)\n",
        "ax1.set_title('Variable Original', fontsize=10)\n",
        "\n",
        "ax2 = axes[1]\n",
        "sns.boxplot(data=y_test_t.values, orient=\"h\", ax=ax2)\n",
        "ax2.set_title('Variable Tratada', fontsize=10)\n",
        "# Ajustar espaciado entre subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar los gráficos\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Csz3jhxZAk1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Model Training, interpretation and Visualization\n",
        ""
      ],
      "metadata": {
        "id": "GdGTphggAmYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import tree\n",
        "\n",
        "# Definiendo modelo\n",
        "dtree = DecisionTreeRegressor(max_depth = 3,\n",
        "                              min_samples_leaf = 0.05,\n",
        "                              random_state = 123)\n",
        "\n",
        "dtree = dtree.fit(X_train[features].fillna(X_train[features].mean()), y_train_t)"
      ],
      "metadata": {
        "id": "CsqHxx0XAoFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ajustar el tamaño de la figura\n",
        "plt.figure(figsize=(60, 20))\n",
        "\n",
        "# Graficar el árbol de decisión\n",
        "tree.plot_tree(dtree,\n",
        "               feature_names=features,\n",
        "               filled=True,\n",
        "               fontsize=20,  # Ajustar el tamaño de la fuente aquí\n",
        "               node_ids=True)  # Opcional: añadir IDs a los nodos para referencia\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "cE3-dA8cArI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Model Evaluation"
      ],
      "metadata": {
        "id": "A64J_urrAuyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_predictions = dtree.predict(X_train[features].fillna(X_train[features].mean()))\n",
        "y_test_predictions  = dtree.predict(X_test[features].fillna(X_train[features].mean()))\n",
        "\n",
        "X_train['prediction'] = y_train_predictions\n",
        "X_test['prediction']  = y_test_predictions"
      ],
      "metadata": {
        "id": "vZk6uk1vAtpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_train_t, y_train_predictions)"
      ],
      "metadata": {
        "id": "KiwOHY5cAy4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test_t,  y_test_predictions)\n",
        ""
      ],
      "metadata": {
        "id": "bSYqr-J6A0ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t.groupby(X_train.codmes_flujo).apply(lambda group: r2_score(group, X_train.prediction.loc[group.index]))\n",
        ""
      ],
      "metadata": {
        "id": "RvmaSNnXA199"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_t.groupby(X_test.codmes_flujo).apply(lambda group: r2_score(group, X_test.prediction.loc[group.index]))\n",
        ""
      ],
      "metadata": {
        "id": "oc0DADLQA3ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la desviación porcentual\n",
        "deviation = (X_test.prediction - y_test_t) / y_test_t * 100\n",
        "\n",
        "# Definir los rangos\n",
        "conditions = [\n",
        "    (deviation >= -25) & (deviation <= 25),\n",
        "    (deviation > 25),\n",
        "    (deviation < -25)\n",
        "]\n",
        "choices = ['b. ±25%', 'a. > 25%', 'c. < -25%']\n",
        "\n",
        "# Categorizar cada caso\n",
        "deviation_category = np.select(conditions, choices)"
      ],
      "metadata": {
        "id": "sVkL7JF2A-a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts_global = pd.Series(deviation_category).value_counts()\n",
        "category_percentages = (category_counts_global / category_counts_global.sum()) * 100\n",
        "\n",
        "category_counts_global_df = pd.DataFrame({\n",
        "    'Count': category_counts_global,\n",
        "    'Percentage': category_percentages})\n",
        "\n",
        "category_counts_global_df"
      ],
      "metadata": {
        "id": "O1vf0L29BA_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame con los resultados\n",
        "df = pd.DataFrame({\n",
        "    'segment': X_test.codmes_flujo,\n",
        "    'category': deviation_category})\n",
        "\n",
        "# Contar los casos por categoría y segmentación\n",
        "category_counts = df.groupby(['segment', 'category']).size().unstack(fill_value=0)\n",
        "category_counts_normalized = category_counts.div(category_counts.sum(axis=1), axis=0)"
      ],
      "metadata": {
        "id": "K2sHRC2JBC5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar las barras apiladas\n",
        "category_counts_normalized.plot(kind='bar', stacked=True, figsize=(8, 4), color=['#204e74', '#7ba2cd', '#dfe7f3'], width=1.0)\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.title('Precision mensual')\n",
        "plt.xlabel('Codmes_flujo', fontsize = 10)\n",
        "plt.ylabel('Proporcion', fontsize = 10)\n",
        "plt.legend(title='Rango', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.xticks(rotation=0, fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "\n",
        "# Ajustar el gráfico\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hgH_OPrqBEsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = pd.DataFrame({'features' : features,\n",
        "                            'importance' : dtree.feature_importances_}).sort_values('importance', ascending = False)\n",
        "importances.loc[importances.importance > 0]"
      ],
      "metadata": {
        "id": "wVShE_fABGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filtrar características con importancia mayor que 0\n",
        "importances_filtered = importances.loc[importances.importance > 0]\n",
        "\n",
        "# Configuración del estilo del gráfico\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Crear la paleta de colores en tonos de azul\n",
        "custom_palette = sns.light_palette(\"#1C75BC\", n_colors=len(importances_filtered), reverse=True)\n",
        "\n",
        "# Crear el gráfico de barras\n",
        "plt.figure(figsize=(8, 3))\n",
        "sns.barplot(x='importance', y='features', data=importances_filtered, palette=custom_palette)\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.title('Decision Tree - Feature Importances', fontsize=10)\n",
        "plt.xlabel('Importance', fontsize=8)\n",
        "plt.ylabel('Features', fontsize=8)\n",
        "\n",
        "# Cambiar el tamaño de las etiquetas de los ticks de los ejes\n",
        "plt.xticks(fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "\n",
        "# Quitar el borde superior y derecho\n",
        "sns.despine()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vD3HK4LcBI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aplicar el árbol a las observaciones para obtener los nodos a los que pertenece cada una\n",
        "nodos = dtree.apply(pddf[features].fillna(pddf[features].mean()))\n",
        "pred = dtree.predict(pddf[features].fillna(pddf[features].mean()))\n",
        "\n",
        "# Crear un DataFrame para almacenar la información\n",
        "df_nodos = pd.DataFrame({'Nodo': nodos,\n",
        "                         'Predictions': np.round(pred, 0),\n",
        "                         'Mes': pddf.codmes_flujo})\n",
        "\n",
        "# Contar cuántas observaciones caen en cada nodo para cada mes\n",
        "stability = df_nodos.groupby(['Mes', 'Predictions']).size().unstack(fill_value=0)"
      ],
      "metadata": {
        "id": "X419qAAOBLKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stability"
      ],
      "metadata": {
        "id": "NGxxEEb1BNnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calcular las frecuencias relativas por mes\n",
        "stability_relative = stability.div(stability.sum(axis=1), axis=0)\n",
        "\n",
        "# Crear el gráfico de barras apiladas con frecuencias relativas y barras pegadas\n",
        "stability_relative.plot(kind='bar', stacked=True, figsize=(10, 5), width=1.0)\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.title('Estabilidad de los nodos del árbol de decisión mes a mes (Frecuencias Relativas)')\n",
        "plt.xlabel('Mes')\n",
        "plt.ylabel('Proporción de observaciones')\n",
        "plt.legend(title='Nodo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Ajustar el gráfico\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKWgPzsxBQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Árboles interactivos"
      ],
      "metadata": {
        "id": "KVKjZGWXBRng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Squared Error between true and predicted values.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true (array-like): Array of true target values.\n",
        "    - y_pred (array-like): Array of predicted values.\n",
        "\n",
        "    Returns:\n",
        "    - squared_error (float): Squared Error between true and predicted values.\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def calculate_squared_error_loss_and_plot(df, variable, target, num_bins=10):\n",
        "    \"\"\"\n",
        "    Calculate the squared error loss function for different cut points of a given variable and generate plots for these calculations.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): Pandas DataFrame containing the dataset.\n",
        "    - variable (str): The name of the variable to evaluate.\n",
        "    - target (str): The name of the target variable.\n",
        "    - num_bins (int, optional): Number of bins to divide the variable into. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "    - min_loss (float): The minimum squared error loss value across all cut points.\n",
        "    - suggested_cut (float): The optimal cut point that yields the minimum loss.\n",
        "    - cut_points (array): Array of cut points used.\n",
        "    - losses (list): List of squared error loss values corresponding to each cut point.\n",
        "    - left_counts (list): List of counts of samples to the left of each cut point.\n",
        "    - right_counts (list): List of counts of samples to the right of each cut point.\n",
        "    - left_means (list): List of mean target values for samples to the left of each cut point.\n",
        "    - right_means (list): List of mean target values for samples to the right of each cut point.\n",
        "    - missing_count (int): Number of missing values in the variable.\n",
        "    - missing_target_mean (float): Mean target value for missing values.\n",
        "    - total_count (int): Total number of observations.\n",
        "    - total_target_mean (float): Mean target value for all observations.\n",
        "    \"\"\"\n",
        "    # Handle missing values\n",
        "    df_clean = df.dropna(subset=[variable])\n",
        "    missing_count = df[variable].isna().sum()\n",
        "    missing_target_mean = df[target][df[variable].isna()].mean()\n",
        "    cut_points = np.percentile(df_clean[variable], np.linspace(0, 100, num_bins + 1))[1:-1]  # Exclude min and max\n",
        "\n",
        "    losses = []\n",
        "    left_counts = []\n",
        "    right_counts = []\n",
        "    left_means = []\n",
        "    right_means = []\n",
        "\n",
        "    y_true = df_clean[target].values\n",
        "\n",
        "    for threshold in cut_points:\n",
        "        mask = df_clean[variable] <= threshold\n",
        "        left_count = np.sum(mask)\n",
        "        right_count = len(y_true) - left_count\n",
        "\n",
        "        if left_count > 0:\n",
        "            left_mean = y_true[mask].mean()\n",
        "            left_pred = np.full_like(y_true[mask], left_mean, dtype=np.float64)\n",
        "            mse_left = squared_error(y_true[mask], left_pred)\n",
        "        else:\n",
        "            mse_left = 0\n",
        "\n",
        "        if right_count > 0:\n",
        "            right_mean = y_true[~mask].mean()\n",
        "            right_pred = np.full_like(y_true[~mask], right_mean, dtype=np.float64)\n",
        "            mse_right = squared_error(y_true[~mask], right_pred)\n",
        "        else:\n",
        "            mse_right = 0\n",
        "\n",
        "        proportion_left = left_count / len(y_true)\n",
        "        proportion_right = right_count / len(y_true)\n",
        "\n",
        "        weighted_mse = (proportion_left * mse_left) + (proportion_right * mse_right)\n",
        "        losses.append(weighted_mse)\n",
        "        left_counts.append(left_count)\n",
        "        right_counts.append(right_count)\n",
        "        left_means.append(left_mean)\n",
        "        right_means.append(right_mean)\n",
        "\n",
        "    min_loss_index = np.argmin(losses)\n",
        "    suggested_cut = cut_points[min_loss_index]\n",
        "\n",
        "    # Total data metrics\n",
        "    total_count = len(df)\n",
        "    total_target_mean = df[target].mean()\n",
        "\n",
        "    return (min(losses), suggested_cut, cut_points, losses, left_counts, right_counts, left_means, right_means,\n",
        "            missing_count, missing_target_mean, total_count, total_target_mean)\n",
        "\n",
        "def plot_best_regression_variable(df, variables, target, num_bins=10):\n",
        "    \"\"\"\n",
        "    Evaluate multiple variables to determine which provides the best split based on the squared error loss function.\n",
        "    Plots the squared error loss function for the variable with the best cut.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): Pandas DataFrame containing the dataset.\n",
        "    - variables (list of str): List of variable names to evaluate.\n",
        "    - target (str): The name of the target variable.\n",
        "    - num_bins (int, optional): Number of bins to divide each variable into. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "    - results_df (DataFrame): DataFrame containing results for all variables evaluated, including:\n",
        "        - 'Variable': The name of the variable.\n",
        "        - 'Min Loss Function': The minimum squared error loss function value.\n",
        "        - 'Suggested Cut': The optimal cut point for that variable.\n",
        "        - 'Left Count': Number of samples to the left of the suggested cut.\n",
        "        - 'Right Count': Number of samples to the right of the suggested cut.\n",
        "        - 'Left Mean': Mean target value to the left of the suggested cut.\n",
        "        - 'Right Mean': Mean target value to the right of the suggested cut.\n",
        "        - 'Missing Count': Number of missing values in the variable.\n",
        "        - 'Missing Target Mean': Mean target value for missing values.\n",
        "        - 'Total Count': Total number of observations.\n",
        "        - 'Total Target Mean': Mean target value for all observations.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for var in variables:\n",
        "        (min_loss, suggested_cut, cut_points, losses, left_counts, right_counts, left_means, right_means,\n",
        "         missing_count, missing_target_mean, total_count, total_target_mean) = calculate_squared_error_loss_and_plot(df, var, target, num_bins)\n",
        "        results.append({\n",
        "            'Variable': var,\n",
        "            'Min Loss Function': min_loss,\n",
        "            'Suggested Cut': suggested_cut,\n",
        "            'Left Count': left_counts[np.argmin(losses)],\n",
        "            'Right Count': right_counts[np.argmin(losses)],\n",
        "            'Left Mean': left_means[np.argmin(losses)],\n",
        "            'Right Mean': right_means[np.argmin(losses)],\n",
        "            'Missing Count': missing_count,\n",
        "            'Missing Target Mean': missing_target_mean,\n",
        "            'Total Count': total_count,\n",
        "            'Total Target Mean': total_target_mean\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values(by='Min Loss Function')  # Sort by Min Loss Function in ascending order\n",
        "\n",
        "    # Identify the best variable\n",
        "    best_variable = results_df.loc[results_df['Min Loss Function'].idxmin()]\n",
        "\n",
        "    best_var = best_variable['Variable']\n",
        "    _, suggested_cut, cut_points, losses, _, _, _, _, _, _, _, _ = calculate_squared_error_loss_and_plot(df, best_var, target, num_bins)\n",
        "\n",
        "    # Plot the squared error loss function for the best variable\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(cut_points, losses, marker='o', linestyle='-', color='b', label='Loss Function')\n",
        "    plt.axvline(x=suggested_cut, color='r', linestyle='--', label='Suggested Cut')\n",
        "    plt.xlabel(f'Percentiles of {best_var}', fontsize=10)\n",
        "    plt.ylabel('Loss Function (Squared Error)', fontsize=10)\n",
        "    plt.title(f'Loss Function vs Percentiles of {best_var} for {target}', fontsize=12)\n",
        "    plt.legend(fontsize=9)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "TJhA1Cm_BVT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.precision', 3)\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)"
      ],
      "metadata": {
        "id": "u9GqLImlBYUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the variables to test\n",
        "variables = ['rat_pas_act_meanprev6_ant_per_med', 'mtoprincipalsol_rat_avg6_avg12_zn_pastrx_med', 'mtosaldopromediovigsol_1000_countprev12_ant_per_deu90k_med', 'mtocuotapresunta_meanprev12_seg_ratio']\n",
        "\n",
        "#variables = features\n",
        "X_train['fad_t'] = y_train_t\n",
        "\n",
        "# Get results and plot with the desired number of bins\n",
        "num_bins = 50  # You can adjust the number of bins here\n",
        "results_df = plot_best_regression_variable(X_train, variables, 'fad_t', num_bins)\n",
        "results_df"
      ],
      "metadata": {
        "id": "JvbTAeuNBZv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Gracias por completar este laboratorio!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-VT-ydWCBdzL"
      }
    }
  ]
}